---
title: "Mid"
author: "周聖諺"
date: "4/27/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
gen_datas <- function(n){
  p_1 <- 0.6
  mu_1 <- 0
  sigma_1 <- 1
  mu_2 <- 3
  sigma_2 <- 1
  
  p <- runif(n, 0, 1)
  
  switch_func <- function(p){
    if(p < p_1){
      # Cluster 1
      return(rnorm(1, mu_1, sigma_1))
    }else{
      # Cluster 2
      return(rnorm(1, mu_2, sigma_2))
    }
  }
  x <- sapply(p, switch_func)
  
  return(x)
}

data <- gen_datas(200)
```

## TargetScore Package

```{r}
library(TargetScore)

X <- c(rnorm(100,mean=-3), rnorm(100,mean=3))
tmp <- vbgmm(data, tol=1e-3)
names(tmp)
print(tmp$label)
```

## Another Implementation

```{r}
suppressPackageStartupMessages(require(matrixcalc))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(mvtnorm))
suppressPackageStartupMessages(library(Matrix))

# Define ggplot2 theme
gg_theme <- function(){
  p <- theme(
      plot.title = element_text(size = 20,face = 'bold',
                                margin = margin(0,0,3,0), hjust = 0.5),
      axis.text = element_text(size = rel(1.05), color = 'black'),
      axis.title = element_text(size = rel(1.45), color = 'black'),
      axis.title.y = element_text(margin = margin(0,10,0,0)),
      axis.title.x = element_text(margin = margin(10,0,0,0)),
      axis.ticks.x = element_line(colour = "black", size = rel(0.8)),
      axis.ticks.y = element_blank(),
      legend.position = "right",
      legend.key.size = unit(1.4, 'lines'),
      legend.title = element_text(size = 12, face = 'bold'),
      legend.text = element_text(size = 12),
      panel.border = element_blank(),
      panel.grid.major = element_line(colour = "gainsboro"),
      panel.background = element_blank()
    )
  return(p)
}

# Mixture density using approximate predictive gaussian distribution
mixture_pdf_gaussian <- function(model, data){
  mixture <- vector(mode = "numeric", length = NROW(data))
  for (k in 1:length(model$nu)) {
    tau_k <- model$W[,,k] * model$nu[k] # TODO: Is this right?
    mu_k  <- model$m[, k]
    mixture <- mixture + 
      model$pi_k[k] * dmvnorm(cbind(data$x,data$y),
                              mean = mu_k, 
                              sigma = solve(tau_k))
  }
  return(mixture)
}
# Mixture density using predictive t-distribution
mixture_pdf_t <- function(model, data){
  mixture <- vector(mode = "numeric", length = NROW(data))
  for (k in 1:length(model$nu)) {
    L_k <- solve((((model$nu[k] + 1 - NROW(model$m)) * model$beta[k]) / 
                    (1 + model$beta[k])) * model$W[,,k])
    mixture <- mixture + (model$alpha[k]/sum(model$alpha)) * 
                dmvt(x = cbind(data$x,data$y), delta = model$m[, k], 
                     sigma = L_k, df = model$nu[k] + 1 - NROW(model$m), 
                     log = FALSE, type = "shifted")
  }
  return(mixture)
}
# Use the log sum exp trick for having numeric stability
log_sum_exp <- function(x) {
  # Computes log(sum(exp(x))
  offset <- max(x)
  s <- log(sum(exp(x - offset))) + offset
  i <- which(!is.finite(s))
  if (length(i) > 0) { s[i] <- offset }
  return(s)
}
```

## E Step

Update $ln \ \rho_{nk}$

$$
ln \ \rho_{nk} = E_{\pi_k}[ln \ \pi_k] − \frac{D}{2} ln(2\pi) + \frac{1}{2} E_{\Lambda_k}[ln \ |\Lambda_k|] − \frac{1}{2} E_{\mu_k, \Lambda_k}[(x_n − \mu_k)^{\top} \Lambda_k (x_n − \mu_k)]
$$

```{r}
update_log_rho_nk <- function(X, k, m_k, log_pi, log_Lambda, D, nu_k, W_k){
  diff <- sweep(X, MARGIN = 2, STATS = m_k[, k], FUN = "-")
  log_rho_nk[, k] <- log_pi[k] + 
                     0.5*log_Lambda[k] - 
                     0.5*(D/log(pi)) - 
                     0.5*nu_k[k] * diag(diff %*% W_k[,,k] %*% t(diff))
  return(log_rho_nk)
}
```

## M Step

### 1. Some Utility Variables

Update $N_k, \bar{x}_k, S_k$

$$
N_k = \sum^N_{n=1} r_{nk}
$$

$$
\bar{x}_k = \frac{1}{N_k} \sum^N_{n=1} r_{nk} x_n
$$

$$
S_k = \frac{1}{N_k} \sum^N_{n=1} r_{nk} (x_n - \bar{x}_k) (x_n - \bar{x}_k)^{\top}
$$

```{r}
update_N_k <- function(r_nk){
  return(colSums(r_nk) + 1e-10)
}

update_x_bar_k <- function(k, r_nk, X, N_k){
  return((r_nk[ ,k] %*% X) / N_k[k])
}

update_S_k <- function(k, X, x_bar_k, r_nk, N_k){
  x_cen <- sweep(X, MARGIN = 2, STATS = x_bar_k[, k], FUN = "-")
  return(t(x_cen) %*% (x_cen * r_nk[, k]) / N_k[k])
}
```

### 2. Variable of Miture Gaussian

Update $ln \ \Lambda_k$

$$
ln \ \bar{\Lambda}_k = E[ln \ |\Lambda_k|] = \sum^D_{d=1} \psi(\frac{\nu_k + 1 - d}{2}) + D \ ln2 + ln \ |W_k|
$$

Update $m_k$

$$
\frac{1}{\beta_k}(\beta_0 m_0 + N_k \bar{x}_k)
$$

Update $ln \ \pi_k$, where $\hat{\alpha} = \sum_k \alpha_k$

$$
ln \ \bar{\pi}_k = E[ln \ \pi_k] = \psi(\alpha_k) - \psi(\hat{\alpha})
$$

```{r}
update_log_lambda_k <- function(k, nu_k, D, w_k){
  return(sum(digamma((nu_k[k] + 1 - 1:D)/2)) + D*log(2) + log(det(W_k[,,k])))
}

update_m_k <- function(k, beta_k, m_0, N_k, x_bar_k){
  return((1/beta_k[k]) * (beta_0*m_0 + N_k[k]*x_bar_k[, k]))
}

update_log_pi <- function(alpha){
  return(digamma(alpha) - digamma(sum(alpha)))
}
```

### 3. Variables of Dirichlet Distribution

Support $x_1, ..., x_K$ where $x_i \in (0, 1)$ and $\sum^K_{i=1} x_i = 1, K > 2$ with parameters $\alpha_1, ..., \alpha_K > 0$

$$
X \sim \mathcal{Dir}(\alpha) = \frac{1}{B(\alpha)} \prod^K_{i=1} x^{\alpha_i - 1}_{i}
$$

where the Beta function $B(\alpha)=\frac{\prod^K_{i=1} \Gamma(\alpha_i)}{\Gamma(\sum^K_{i=1} \alpha_i)}$ and $\alpha$ and $X$ are a set of random variables that $\alpha = \{ \alpha_1, ..., \alpha_K\}$ and $X = \{ X_1, ..., X_K\}$. Note that $x_i$ is a sample value generated by $X_i$.

The mean and the variance of the Dirichlet distribution is

$$
E[X_i] = \frac{\alpha_i}{\sum^K_{k=1} \alpha_k}
$$

$$
Var[ln \ X_i] = \psi(\alpha_i) - \psi(\sum^K_{k=1} \alpha_k)
$$

where $\psi$ is **digamma** function 

$$
\psi(x) = \frac{d}{dx} ln(\Gamma(x)) = \frac{\Gamma'(x)}{\Gamma(x)} \approx ln(x) - \frac{1}{2x}
$$

In order to reduce the number of parameters, we use **Symmetric Dirichlet distribution** which is a  special form of Dirichlet distribution that defined as the following

$$
X \sim \mathcal{SymmDir}(\alpha_0) = \frac{\Gamma(\alpha_0 K)}{\Gamma(\alpha_0)^K} \prod^K_{i=1} x^{\alpha_0-1}_i = f(x_1, ..., x_{K-1}; \alpha_0)
$$
where $X = \{ X_1, ..., X_{K-1} \}$. The $\alpha$ parameter of the symmetric Dirichlet is a scalar which means all the elements $\alpha_i$ of the $\alpha$ are the same $\alpha = \{ \alpha_0, ..., \alpha_0 \}$. With this property, we can greatly reduce the number of the parameters of Dirichlet distribution.

Thus, we can model the distribution of the weights of mixture Gaussian as a symmetric Dirichlet distribution.

$$
p(\pi) = \mathcal{Dir}(\pi | \alpha_0) = \frac{1}{B(\alpha_0)} \prod^K_{k=1} \pi^{\alpha_0 - 1}_{k} = C(\alpha_0) \prod^K_{k=1} \pi^{\alpha_0 - 1}_{k}
$$

### 4. Variables of Wishart Distribution

The parameters of mixture Gaussian follow the distribution of Wishart distribution. It is called Gaussian-Wishart distribution 

$$
p(\mu, \Lambda) = p(\mu | \Lambda) p(\Lambda) = \prod^K_{k=1} \mathcal{N}(\mu_k | m_0, (\beta_0 \Lambda_k)^{-1}) \mathcal{W}(\Lambda_k | W_0, \nu_0)
$$

Update $W_k$

$$
W^{-1}_k = W^{-1}_0 + N_k S_k + \frac{\beta_0 N_k}{\beta_0 + N_k} (\bar{x}_k - m_0) (\bar{x}_k - m_0)^{\top}
$$

Update $\nu_k$

$$
\nu_k = \nu_k + N_k + 1
$$

```{r}
update_w_k <- function(k, W_0_inv, N_k, S_k, beta_0, x_bar_k, m_0){
  return(solve(W_0_inv + N_k[k] * S_k[,,k] + ((beta_0*N_k[k])/(beta_0 + N_k[k])) * tcrossprod((x_bar_k[, k] - m_0))))
}

update_nu_k <- function(nu_0, N_k){
  return(nu_0 + N_k + 1)
}
```

Update $\beta_k$

$$
\beta_k = \beta_0 + N_k
$$

```{r}
update_beta_k <- function(beta_0, N_k){
  return(beta_0 + N_k)
}
```



```{r}
# Fit VBLR model
vb_gmm <- function(X, 
                   K = 3, 
                   alpha_0 = 1/K, 
                   m_0 = c(colMeans(X)), 
                   beta_0 = 1, 
                   nu_0 = NCOL(X) + 50, 
                   W_0 = diag(100, NCOL(X)), 
                   max_iter = 500, 
                   epsilon_conv = 1e-4, 
                   is_animation = FALSE, 
                   is_verbose = FALSE){
  
  # Compute logB function
  logB <- function(W, nu){
    D <- NCOL(W)
    return(-0.5*nu*log(det(W)) - 
          (0.5*nu*D*log(2) + 
          0.25*D*(D - 1) * log(pi) + 
          sum(lgamma(0.5 * (nu + 1 - 1:NCOL(W)))) ))  #log of B.79
  }
  
  X <- as.matrix(X)
  D <- NCOL(X)              # Number of features
  N <- NROW(X)              # Number of observations
  W_0_inv <- solve(W_0)     # Compute W^{-1}
  L <- rep(-Inf, max_iter)  # Store the lower bounds
  r_nk = log_r_nk = log_rho_nk <- matrix(0, nrow = N, ncol = K)
  x_bar_k <- matrix(0, nrow = D, ncol = K)
  S_k = W_k <- array(0, c(D, D, K ) )
  log_pi = log_Lambda <- rep(0, K)
  
  if (is_animation) {
    # Variables needed for plotting
    dt <- data.table(expand.grid(x = seq(from = min(X[,1]) - 2, 
                                         to = max(X[,1]) + 2, 
                                         length.out = 80), 
                                 y = seq(from = min(X[,2]) - 8, 
                                         to = max(X[,2]) + 2, 
                                         length.out = 80)))
  }
  dt_all <- data.table(x = numeric(), 
                       y = numeric(), 
                       z = numeric(), 
                       iter = numeric())
    
  m_k     <- t(kmeans(X, K, nstart = 25)$centers)  # Mean of Gaussian Wishart
  beta_k  <- rep(beta_0, K)                # Scale of precision matrix, Gaussian Wishart
  nu_k    <- rep(nu_0, K)                  # Degrees of freedom, Wishart
  alpha   <- rep(alpha_0, K)               # Dirichlet parameter
  log_pi  <- digamma(alpha) - digamma(sum(alpha))
  for (k in 1:K) {
    W_k[,,k] <-  W_0  # Scale matrix for Wishart
    log_Lambda[k] <- sum(digamma((nu_k[k] + 1 - c(1:D))/2)) + 
                                  D*log(2) + log(det(W_k[,,k]))
  }
  
  if (is_animation) { # Create animation for initial assignments
    my_z = mixture_pdf_t(model = list(m = m_k, 
                                      W = W_k, 
                                      beta = beta_k, 
                                      nu = nu_k, 
                                      alpha = rep(1/K, K)), 
                        data = dt)
    dt_all <- rbind(dt_all, dt[, z := my_z] %>% .[, iter := 0])
  }
  
  # Iterate to find optimal parameters
  for (i in 2:max_iter) {
    ##-------------------------------
    # Variational E-Step
    ##-------------------------------
    for (k in 1:K) {
      diff <- sweep(X, MARGIN = 2, STATS = m_k[, k], FUN = "-")
      log_rho_nk[, k] <- log_pi[k] + 
                         0.5*log_Lambda[k] - 
                         0.5*(D/log(pi)) - 
                         0.5*nu_k[k] * diag(diff %*% W_k[,,k] %*% t(diff)) # log of 10.67
    }
    # Responsibilities using the logSumExp for numerical stability
    Z        <- apply(log_rho_nk, 1, log_sum_exp)
    log_r_nk <- log_rho_nk - Z              # log of 10.49
    r_nk     <- apply(log_r_nk, 2, exp)     # 10.49
    
    ##-------------------------------
    # Variational M-Step
    ##-------------------------------
    N_k <- colSums(r_nk) + 1e-10  # 10.51
    for (k in 1:K) {
      x_bar_k[, k] <- (r_nk[ ,k] %*% X) / N_k[k]   # 10.52
      x_cen        <- sweep(X,MARGIN = 2,STATS = x_bar_k[, k],FUN = "-")
      S_k[, , k]   <- t(x_cen) %*% (x_cen * r_nk[, k]) / N_k[k]  # 10.53
    }
    # Update Dirichlet parameter
    alpha <- alpha_0 + N_k  # 10.58
    # # Compute expected value of mixing proportions
    pi_k <- (alpha_0 + N_k) / (K * alpha_0 + N)
    # Update parameters for Gaussia-nWishart distribution
    beta_k <- beta_0 + N_k    # 10.60
    nu_k   <- nu_0 + N_k + 1  # 10.63
    for (k in 1:K) {
      # 10.61
      m_k[, k]   <- (1/beta_k[k]) * (beta_0*m_0 + N_k[k]*x_bar_k[, k])  
      # 10.62
      W_k[, , k] <- W_0_inv + 
                    N_k[k] * S_k[,,k] + 
                    ((beta_0*N_k[k])/(beta_0 + N_k[k])) * tcrossprod((x_bar_k[, k] - m_0))    
      W_k[, , k] <- solve(W_k[, , k])
    }
    # Update expectations over \pi and \Lambda
    # 10.66
    log_pi <- digamma(alpha) - digamma(sum(alpha))
    for (k in 1:K) { # 10.65                                              
      log_Lambda[k] <- sum(digamma((nu_k[k] + 1 - 1:D)/2)) + 
          D*log(2) + log(det(W_k[,,k])) 
    }
    
    ##-------------------------------
    # Variational lower bound
    ##-------------------------------
    lb_px = lb_pml = lb_pml2 = lb_qml <- 0
    for (k in 1:K) {
      # 10.71
      lb_px <- lb_px + N_k[k] * (log_Lambda[k] - D/beta_k[k] - nu_k[k] * 
          matrix.trace(S_k[,,k] %*% W_k[,,k]) - nu_k[k]*t(x_bar_k[,k] - 
          m_k[,k]) %*% W_k[,,k] %*% (x_bar_k[,k] - m_k[,k]) - D*log(2*pi) ) 
      # 10.74
      lb_pml <- lb_pml + D*log(beta_0/(2*pi)) + log_Lambda[k] - 
          (D*beta_0)/beta_k[k] - beta_0*nu_k[k]*t(m_k[,k] - m_0) %*% 
          W_k[,,k] %*% (m_k[,k] - m_0)    
      # 10.74
      lb_pml2 <- lb_pml2 + nu_k[k] * matrix.trace(W_0_inv %*% W_k[,,k]) 
      # 10.77
      lb_qml <- lb_qml + 0.5*log_Lambda[k] + 0.5*D*log(beta_k[k]/(2*pi)) - 
          0.5*D - (-logB(W = W_k[,,k], nu = nu_k[k]) - 
          0.5*(nu_k[k] - D - 1)*log_Lambda[k] + 0.5*nu_k[k]*D)
    }
    lb_px  <- 0.5 * lb_px             # 10.71
    lb_pml <- 0.5*lb_pml + K*logB(W = W_0,nu = nu_0) + 0.5*(nu_0 - D - 1) * 
        sum(log_Lambda) - 0.5*lb_pml2 # 10.74
    lb_pz  <- sum(r_nk %*% log_pi)    # 10.72
    lb_qz  <- sum(r_nk * log_r_nk)    # 10.75
    lb_pp  <- sum((alpha_0 - 1)*log_pi) + lgamma(sum(K*alpha_0)) -
        K*sum(lgamma(alpha_0))        # 10.73
    lb_qp  <- sum((alpha - 1)*log_pi) + lgamma(sum(alpha)) - 
        sum(lgamma(alpha)) # 10.76
    # Sum all parts to compute lower bound
    L[i] <- lb_px + lb_pz + lb_pp + lb_pml - lb_qz - lb_qp - lb_qml
    
    ##-------------------------------
    # Evaluate mixture density for plotting
    ##-------------------------------
    if (is_animation) {
      if ( (i - 1) %% 5 == 0 | i < 10) {
        my_z = mixture_pdf_t(model = list(m = m_k, W = W_k, beta = beta_k, 
                nu = nu_k, alpha = alpha), data = dt)
        dt_all <- rbind(dt_all, dt[, z := my_z] %>% .[, iter := i - 1])
      }
    }
    # Show VB difference
    if (is_verbose) { cat("It:\t",i,"\tLB:\t",L[i],
                          "\tLB_diff:\t",L[i] - L[i - 1],"\n")}
    # Check if lower bound decreases
    if (L[i] < L[i - 1]) { message("Warning: Lower bound decreases!\n"); }
    # Check for convergence
    if (abs(L[i] - L[i - 1]) < epsilon_conv) { break }
    # Check if VB converged in the given maximum iterations
    if (i == max_iter) {warning("VB did not converge!\n")}
  }
  obj <- structure(list(X = X, K = K, N = N, D = D, pi_k = pi_k, 
                        alpha = alpha, r_nk = r_nk,  m = m_k, W = W_k, 
                        beta = beta_k, nu = nu_k, L = L[2:i], 
                        dt_all = dt_all), class = "vb_gmm")
  return(obj)
}
```

```{r}
library(tidyverse)  # data manipulation

data <- iris
print(data)
X <- as.matrix(data[c(3, 4)])
```

```{r}
set.seed(1)  # For reproducibility
K <- 5      # Number of clusters
# Run vb-gmm model model
vb_gmm_model <- vb_gmm(X = X, 
                       K = K, 
                       alpha_0 = 1e-5, 
                       max_iter = 1001, 
                       is_animation = TRUE, 
                       is_verbose = FALSE)
```

```{r}
data.grid <- expand.grid(x = seq(from = min(X[,1]) - 2, 
                                 to = max(X[,1]) + 2, length.out = 100), 
                         y = seq(from = min(X[,2]) - 8, 
                                 to = max(X[,2]) + 2, length.out = 100))
q.samp <- cbind(data.grid, z = mixture_pdf_t(vb_gmm_model,data.grid))
ggplot() + 
  geom_point(data = data.frame(X), mapping = aes(Petal.Length, Petal.Width)) + 
  geom_contour(data = q.samp, mapping = aes(x = x,y = y, z = z, 
               colour = ..level..), binwidth = 0.001) + 
  gg_theme()

```

```{r}
suppressPackageStartupMessages(library(gganimate))
dt <- vb_gmm_model$dt_all %>% .[, iter := as.factor(iter)]
p <- ggplot() + geom_point(data = data.frame(X), aes(Petal.Length, Petal.Width)) + 
  geom_contour(data = dt, mapping = aes(x = x, y = y, z = z, 
               colour = ..level..), binwidth = 0.001) + 
  transition_manual(iter) +
  gg_theme()
p

```