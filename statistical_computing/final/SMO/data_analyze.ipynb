{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from operator import index\n","import numpy as np\n","from numpy.core import numeric\n","import pandas as pd\n","from IPython.display import display\n","\n","import re\n","from bs4 import BeautifulSoup\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","\n","from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_tags(text):\n","    # remove HTML tags\n","    text = BeautifulSoup(text, 'html.parser').get_text()\n","    \n","    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n","    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n","    emoticons = re.findall(r, text)\n","    text = re.sub(r, '', text)\n","    \n","    # convert to lowercase and append all emoticons behind (with space in between)\n","    # replace('-','') removes nose of emoticons\n","    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n","    return text\n","\n","nltk.download('stopwords')\n","def tokenizer_stem_nostop(text):\n","    stop = stopwords.words('english')\n","    porter = PorterStemmer()\n","    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n","            if w not in stop and re.match('[a-zA-Z]+', w)]\n","\n","def preprocess(text):\n","    text = remove_tags(text)\n","    text = tokenizer_stem_nostop(text)\n","    text = ' '.join(text)\n","    return text\n","\n","def read_original():\n","    return pd.read_csv('Womens Clothing E-Commerce Reviews.csv')\n","\n","def prep_save():\n","    df = read_original()\n","    # Remove Null values\n","    df.drop(['Unnamed: 0', 'Title'], axis=1, inplace=True)\n","    df.dropna(inplace=True)\n","    df.to_pickle(\"no_null.pkl\")\n","\n","    # Stemming & Remove stop words\n","    df['Review Text'] = df['Review Text'].apply(preprocess)\n","\n","    print(df.isnull().sum())\n","    df.head()\n","    df.to_pickle(\"clean.pkl\")\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","def vec_lda(review_text, n_comp):\n","    # n_comp = 5\n","    tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, norm='l2')\n","    count = CountVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n","    lda = LatentDirichletAllocation(n_components = n_comp, random_state = 0)\n","\n","    # vec_text = count.fit_transform(review_text)\n","    vec_text = np.array(tfidf.fit_transform(review_text).toarray())\n","    clustered_text = lda.fit_transform(vec_text)\n","\n","    pd.DataFrame(clustered_text).head()\n","\n","    return clustered_text, vec_text, tfidf, lda"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.ticker import PercentFormatter\n","def plot_top_words(model, tfidf, n_top_words, title):\n","    feature_names = tfidf.get_feature_names()\n","    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n","    axes = axes.flatten()\n","    for topic_idx, topic in enumerate(model.components_):\n","        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n","        top_features = [feature_names[i] for i in top_features_ind]\n","        weights = topic[top_features_ind]\n","\n","        ax = axes[topic_idx]\n","        ax.barh(top_features, weights, height=0.7)\n","        ax.set_title(f'Topic {topic_idx +1}',\n","                     fontdict={'fontsize': 30})\n","        ax.invert_yaxis()\n","        ax.tick_params(axis='both', which='major', labelsize=20)\n","        for i in 'top right left'.split():\n","            ax.spines[i].set_visible(False)\n","        fig.suptitle(title, fontsize=40)\n","\n","    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","def plot_dist(df):\n","    # Age\n","    sns.distplot(x=df['Age'], bins=50, color='#9966ff', kde_kws={'bw':0.5})\n","    plt.title('Age Distribution', size=15)\n","    plt.xlabel('Age')\n","    plt.show()\n","\n","    # Rating\n","    sns.distplot(x=df['Rating'], bins=5, color='#3399ff', kde=False, norm_hist=True)\n","    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n","    plt.title('Rating Distribution', size=15)\n","    plt.xticks(range(1, 6))\n","    plt.xlabel('Rating')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Read Dataset\n","is_read_original = False\n","if is_read_original:\n","    df = prep_save()\n","else:\n","    df = pd.read_pickle(\"clean.pkl\")\n","    df_original = pd.read_pickle(\"no_null.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = df[df[\"Recommended IND\"] == 1]\n","# df = df[df[\"Rating\"] == 1]\n","\n","# df = df[df[\"Department Name\"] == \"Intimate\"]\n","\n","# df = df[df[\"Class Name\"] == \"Blouses\"]\n","# df = df[df[\"Class Name\"] == \"Bottoms\"]\n","\n","# df = df[df[\"Age\"] >= 60]\n","# df = df[df[\"Age\"] < 60]\n","\n","df.head()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Vectorization-LDA\n","clustered_text, vec_text, vectorizer, lda = vec_lda(df['Review Text'], n_comp=5)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Plot distribution of properties\n","# plot_dist(df)\n","\n","# Average age & rating of each group\n","groups = ['Division Name', 'Department Name', 'Class Name']\n","fields = ['Age', 'Rating']\n","\n","for group in groups:\n","    for field in fields:\n","        print(\"Group By: \", group)\n","        display(pd.DataFrame(df[[group, field]]).groupby(group).mean())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Vectorizer features\n","summary_text = np.sum(np.array(vec_text.toarray()), axis=0)\n","vectorizer._validate_vocabulary()\n","display(pd.DataFrame(summary_text, index=vectorizer.get_feature_names()).nlargest(10, columns=[0]))\n","\n","# Top words of LDA topics\n","# n_top_words = 10\n","# plot_top_words(lda, tfidf, n_top_words, 'Topics in LDA model')\n","\n","# Word cloud\n","# cloud = WordCloud().generate(\" \".join(list(df['Review Text'])))\n","# cloud.to_file('output.png')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# df_lda = pd.DataFrame(clustered_text)\n","# display(df_lda)\n","# df_txt = pd.DataFrame(df_original['Review Text'].iloc[np.argmax(clustered_text, axis=1)== 1])\n","# df_txt.to_csv(\"topic2_reviews.csv\")\n","# display(df_txt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from svm import SVM as SVM, acc, draw_boundary\n","\n","svm = SVM()\n","X, y = np.array(vec_text.toarray()), df[\"Recommended IND\"].to_numpy()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)\n","\n","svm.fit(X_train, y_train, C=1, max_iter=1000, kernel_type=\"rbf\", gamma=2)\n","\n","train_pred, test_pred, train_acc, test_acc = acc(X_train, X_test, y_train, y_test)\n","\n","display(test_pred[:10])\n","\n","# draw_boundary(X_train, y_train, svm.alpha, svm.b)\n","draw_boundary([(X, y)], [svm], [\"RBF SVM\"])\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}